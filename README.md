# word2vec-skipgram-numpy
Simple NumPy implementation of Word2Vec (skip-gram) for learning word embeddings from scratch.
–ü—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π —Å –Ω—É–ª—è —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º **Word2Vec (skip-gram)** –Ω–∞ –±–∞–∑–µ `NumPy`.  
–ü—Ä–æ–µ–∫—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –º–æ–∂–Ω–æ –æ–±—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤ –±–µ–∑ –≥–æ—Ç–æ–≤—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ (PyTorch, TensorFlow).

## üìå –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- –û—á–∏—Å—Ç–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∫–æ—Ä–ø—É—Å–∞
- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è skip-gram
- –û–±—É—á–µ–Ω–∏–µ skip-gram –º–æ–¥–µ–ª–∏ (negative sampling optional)
- –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤
- –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É

## üöÄ –ü—Ä–∏–º–µ—Ä
```python
from src.model import SkipGram
from src.utils import most_similar

model = SkipGram(vocab_size=5000, embedding_dim=50)
model.train(corpus="data/ruwiki_sample.txt", epochs=3, learning_rate=0.01)

print(most_similar("–º–æ—Å–∫–≤–∞", model, top_n=5))
