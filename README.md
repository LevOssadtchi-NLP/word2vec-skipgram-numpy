# word2vec-skipgram-numpy

Это реализация модели **Word2Vec** архитектуры Skip-gram, созданная с нуля на чистом NumPy. Проект демонстрирует, как можно обучить векторные представления слов (эмбеддинги) на текстовом корпусе, используя только базовые библиотеки Python.

-----

## Основные особенности

  - **Минимальная зависимость**: Используются только `numpy` и `tqdm` (для индикатора прогресса), что позволяет сфокусироваться на алгоритмической сути модели.
  - **Архитектура Skip-gram**: Модель обучается предсказывать контекстные слова, находящиеся в окне вокруг целевого слова.
  - **Negative Sampling**: Реализована оптимизация с помощью Negative Sampling для ускорения обучения по сравнению с полным Softmax.
  - **Гибкость**: Легко настраиваемые параметры, такие как размерность эмбеддингов, размер окна и количество негативных примеров.
  - **Визуализация**: Включен скрипт для визуализации полученных эмбеддингов с использованием t-SNE.

-----

## Структура проекта

[cite\_start]Проект организован в соответствии со стандартными практиками, что делает его удобным для навигации[cite: 1, 2, 3].

```
word2vec-skipgram-numpy/
│
├── data/
│   └── ruwiki_sample.txt           # Пример текстового корпуса
│
├── src/
│   ├── preprocess.py               # Токенизация, очистка текста, построение словаря
│   ├── dataset.py                  # Генерация пар (target, context)
│   ├── model.py                    # Реализация Skip-gram модели на NumPy
│   ├── train.py                    # Основной скрипт для обучения
│   └── utils.py                    # Вспомогательные функции (сохранение/загрузка эмбеддингов)
│
├── notebooks/
│   └── demo.ipynb                  # Jupyter Notebook для демонстрации обучения
│
├── tests/
│   └── visualize.py                # Скрипт для визуализации эмбеддингов
│
├── requirements.txt                # Список зависимостей
├── README.md                       # Этот файл
└── LICENSE
```

-----

##  Установка

1.  **Клонируйте репозиторий:**

    ```bash
    git clone https://github.com/your-username/word2vec-skipgram-numpy.git
    cd word2vec-skipgram-numpy
    ```

2.  **Установите зависимости:**

    ```bash
    pip install -r requirements.txt
    ```

    [cite\_start]Вам понадобятся `numpy`, `tqdm`, `pandas`, `scikit-learn` и `matplotlib`[cite: 3].

-----

## Использование

### 1\. Подготовка данных

Поместите ваш текстовый корпус в папку `data/`. По умолчанию используется `ruwiki_sample.txt`.

### 2\. Обучение модели

Запустите скрипт `train.py` из директории `src/`:

```bash
python src/train.py
```

Этот скрипт выполняет следующие шаги:

1.  Загружает и преобразует текст (`preprocess.py`).
2.  Генерирует обучающие пары (`dataset.py`).
3.  Создает и обучает модель Skip-gram (`model.py`).
4.  Сохраняет полученные эмбеддинги в файл `artifacts/embeddings.csv` (`utils.py`).

### 3\. Визуализация эмбеддингов

После обучения вы можете визуализировать полученные векторы, чтобы увидеть, как семантически близкие слова группируются вместе. Запустите скрипт `visualize.py` из папки `tests/`:

```bash
python tests/visualize.py
```

## Скрипт использует алгоритм **t-SNE** для уменьшения размерности векторов до 2D и строит график.

## Детали реализации

  - **`preprocess.py`**: Класс `Preprocessor` отвечает за очистку текста (удаление лишних символов, приведение к нижнему регистру) и токенизацию. [cite\_start]Он также строит словарь, используя `collections.Counter` и выделяя наиболее частые слова, и заменяет редкие слова на `<UNK>` (Unknown) токен[cite: 1].
  - **`dataset.py`**: Класс `SkipGramDataset` создает пары `(target, context)` из закодированного текста, используя заданный размер окна. [cite\_start]Метод `get_batches` предоставляет генератор, который выдает батчи для обучения[cite: 1].
  - [cite\_start]**`model.py`**: Класс `SkipGram` содержит основную логику модели[cite: 1]:
      - **`__init__`**: Инициализирует матрицы весов `W` (векторы слов) и `W_prime` (векторы контекстов).
      - **`forward`**: Прямой проход, который просто извлекает вектор целевого слова.
      - **`backward`**: Реализует обратное распространение ошибки. Здесь используется **Negative Sampling**: для каждого положительного примера `(target, context)` выбирается несколько случайных негативных примеров.
      - **`train`**: Основной цикл обучения, который проходит по батчам данных, выполняет прямой и обратный проход и обновляет веса.
  - **`utils.py`**: Содержит функции для сохранения и загрузки эмбеддингов в формате CSV.
  - [cite\_start]**`train.py`**: Центральный скрипт, который связывает все компоненты для запуска полного цикла обучения[cite: 2].
  - [cite\_start]**`visualize.py`**: Скрипт для визуализации[cite: 3]. Он загружает CSV-файл с эмбеддингами, применяет t-SNE для уменьшения размерности и строит график с подписями слов.

-----

## Будущие улучшения

  - Реализовать архитектуру **CBOW**.
  - Добавить возможность поиска ближайших соседей для слова.
  - Использовать более эффективные методы негативного сэмплирования, например, на основе частотности слов.
  - Добавить юнит-тесты для основных компонентов.
