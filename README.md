# word2vec-skipgram-numpy

Это реализация модели **Word2Vec** архитектуры Skip-gram, созданная с нуля на чистом NumPy. Проект демонстрирует, как можно обучить векторные представления слов (эмбеддинги) на текстовом корпусе, используя только базовые библиотеки Python.


## Основные особенности

  - **Минимальная зависимость**: Из ключевых зависимостей используются только `numpy`, что позволяет сфокусироваться на алгоритмической сути модели.
  - **Архитектура Skip-gram**: Модель обучается предсказывать контекстные слова, находящиеся в окне вокруг целевого слова.
  - **Negative Sampling**: Реализована оптимизация с помощью Negative Sampling для ускорения обучения по сравнению с полным Softmax.
  - **Гибкость**: Легко настраиваемые параметры, такие как размерность эмбеддингов, размер окна и количество негативных примеров.
  - **Визуализация**: Включен скрипт для визуализации полученных эмбеддингов с использованием t-SNE.


## Структура проекта

Проект организован в соответствии со стандартными практиками, что делает его удобным для навигации.

```
word2vec-skipgram-numpy/
│
├── data/
│   └── ruwiki_sample.txt           # Пример текстового корпуса
│
├── src/
│   ├── preprocess.py               # Токенизация, очистка текста, построение словаря
│   ├── dataset.py                  # Генерация пар (target, context)
│   ├── model.py                    # Реализация Skip-gram модели на NumPy
│   ├── train.py                    # Основной скрипт для обучения
│   └── utils.py                    # Вспомогательные функции (сохранение/загрузка эмбеддингов)
│
├── notebooks/
│   └── demo.ipynb                  # Jupyter Notebook для демонстрации обучения
│
├── tests/
│   └── visualize.py                # Скрипт для визуализации эмбеддингов
│
├── requirements.txt                # Список зависимостей
├── README.md                       # Этот файл
└── LICENSE
```


##  Установка

1.  **Клонируйте репозиторий:**

    ```bash
    git clone https://github.com/your-username/word2vec-skipgram-numpy.git
    cd word2vec-skipgram-numpy
    ```

2.  **Установите зависимости:**

    ```bash
    pip install -r requirements.txt
    ```

    Вам понадобятся `numpy`, `tqdm`, `pandas`, `scikit-learn` и `matplotlib`.


## Использование

### 1\. Подготовка данных

Поместите ваш текстовый корпус в папку `data/`. По умолчанию используется `ruwiki_sample.txt`.

### 2\. Обучение модели

Запустите скрипт `train.py` из директории `src/`:

```bash
python src/train.py
```

Этот скрипт выполняет следующие шаги:

1.  Загружает и преобразует текст (`preprocess.py`).
2.  Генерирует обучающие пары (`dataset.py`).
3.  Создает и обучает модель Skip-gram (`model.py`).
4.  Сохраняет полученные эмбеддинги в файл `artifacts/embeddings.csv` (`utils.py`).


### 3\. Визуализация эмбеддингов

После обучения вы можете визуализировать полученные векторы, чтобы увидеть, как семантически близкие слова группируются вместе. Запустите скрипт `visualize.py` из папки `tests/`:

```bash
python tests/visualize.py
```

### Скрипт использует алгоритм **t-SNE** для уменьшения размерности векторов до 2D и строит график.
<img width="1200" height="800" alt="Figure_22" src="https://github.com/user-attachments/assets/daf44d2a-6847-4356-aa4d-13bf68bfb675" />

## Детали реализации

  - **`preprocess.py`**: Класс `Preprocessor` отвечает за очистку текста (удаление лишних символов, приведение к нижнему регистру) и токенизацию. Он также строит словарь, используя `collections.Counter` и выделяя наиболее частые слова, и заменяет редкие слова на `<UNK>` (Unknown) токен.
  - **`dataset.py`**: Класс `SkipGramDataset` создает пары `(target, context)` из закодированного текста, используя заданный размер окна. Метод `get_batches` предоставляет генератор, который выдает батчи для обучения.
  - **`model.py`**: Класс `SkipGram` содержит основную логику модели:
      - **`__init__`**: Инициализирует матрицы весов `W` (векторы слов) и `W_prime` (векторы контекстов).
      - **`forward`**: Прямой проход, который просто извлекает вектор целевого слова.
      - **`backward`**: Реализует обратное распространение ошибки. Здесь используется **Negative Sampling**: для каждого положительного примера `(target, context)` выбирается несколько случайных негативных примеров.
      - **`train`**: Основной цикл обучения, который проходит по батчам данных, выполняет прямой и обратный проход и обновляет веса.
  - **`utils.py`**: Содержит функции для сохранения и загрузки эмбеддингов в формате CSV.
  - **`train.py`**: Центральный скрипт, который связывает все компоненты для запуска полного цикла обучения.
  - **`visualize.py`**: Скрипт для визуализации. Он загружает CSV-файл с эмбеддингами, применяет t-SNE для уменьшения размерности и строит график с подписями слов.
