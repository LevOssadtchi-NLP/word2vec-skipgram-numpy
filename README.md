# word2vec-skipgram-numpy

Это реализация модели **Word2Vec** архитектуры Skip-gram, созданная с нуля на чистом NumPy. Проект демонстрирует, как можно обучить векторные представления слов (эмбеддинги) на текстовом корпусе, используя только базовые библиотеки Python.


## Основные особенности

  - **Минимальная зависимость**: Из ключевых зависимостей используется только `numpy`, что позволяет сфокусироваться на алгоритмической сути модели.
  - **Архитектура Skip-gram**: Модель обучается предсказывать контекстные слова, находящиеся в окне вокруг целевого слова.
  - **Negative Sampling**: Реализована оптимизация с помощью Negative Sampling для ускорения обучения по сравнению с полным Softmax.
  - **Гибкость**: Легко настраиваемые параметры, такие как размерность эмбеддингов, размер окна и количество негативных примеров.
  - **Визуализация**: Включен скрипт для визуализации полученных эмбеддингов с использованием t-SNE.


## Структура проекта

Проект организован в соответствии со стандартными практиками, что делает его удобным для навигации.

```
word2vec-skipgram-numpy/
│
├── data/
│   └── ruwiki_sample.txt        # корпус (или часть корпуса для примеров)
│
├── src/
│   ├── preprocess.py             # токенизация, словарь, преобразование слов в индексы
│   ├── dataset.py                # генерация пар (target, context) для skip-gram
│   ├── model.py                  # реализация Skip-gram на NumPy
│   ├── train.py                  # цикл обучения
│   └── utils.py                  # функции: сохранение/загрузка эмбеддингов, косинусное сходство
│
├── tests/
│   └── visualize.py             # базовый тест: визуализация эмбеддингов в сниженной размерности
│
├── requirements.txt
├── README.md
└── LICENSE

```


##  Установка

1.  **Клонируйте репозиторий:**

    ```bash
    git clone https://github.com/LevOssadtchi-NLP/word2vec-skipgram-numpy
    cd word2vec-skipgram-numpy
    ```

2.  **Установите зависимости:**

    ```bash
    pip install -r requirements.txt
    ```

    Вам понадобятся `numpy`, `tqdm`, `pandas`, `scikit-learn` и `matplotlib`.


## Использование

### 1\. Подготовка данных

Поместите ваш текстовый корпус в папку `data/`. По умолчанию используется `ruwiki_sample.txt`.

### 2\. Обучение модели

Запустите скрипт `train.py` из директории `src/`:

```bash
python src/train.py
```

Этот скрипт выполняет следующие шаги:

1.  Загружает и преобразует текст (`preprocess.py`).
2.  Генерирует обучающие пары (`dataset.py`).
3.  Создает и обучает модель Skip-gram (`model.py`).
4.  Сохраняет полученные эмбеддинги в файл `artifacts/embeddings.csv` (`utils.py`).


### 3\. Визуализация эмбеддингов

После обучения вы можете визуализировать полученные векторы, чтобы увидеть, как семантически близкие слова группируются вместе. Запустите скрипт `visualize.py` из папки `tests/`:

```bash
python tests/visualize.py
```

**Скрипт использует алгоритм **t-SNE** для уменьшения размерности векторов до 2D и строит график.**
Отчетливо видны области семантически схожих слов.
<img width="1512" height="909" alt="Frame 50" src="https://github.com/user-attachments/assets/d9147fd2-cb9a-4bb5-bce1-e4892b525ea8" />
Также, несмотря на нелинейное снижение размерности (методом t-SNE) хорошо прослеживаются направления семантических градаций.
<img width="1512" height="909" alt="Frame 51" src="https://github.com/user-attachments/assets/f32d97ee-3580-4c21-b88c-c7dd4c1985e3" />
<img width="1512" height="909" alt="Frame 52" src="https://github.com/user-attachments/assets/eab3ad67-02d7-4c32-861e-3a5b06eff797" />


## Детали реализации

  - **`preprocess.py`**: Класс `Preprocessor` отвечает за очистку текста (удаление лишних символов, приведение к нижнему регистру) и токенизацию. Он также строит словарь, используя `collections.Counter` и выделяя наиболее частые слова, и заменяет редкие слова на `<UNK>` (Unknown) токен.
  - **`dataset.py`**: Класс `SkipGramDataset` создает пары `(target, context)` из закодированного текста, используя заданный размер окна. Метод `get_batches` предоставляет генератор, который выдает батчи для обучения.
  - **`model.py`**: Класс `SkipGram` содержит основную логику модели:
      - **`__init__`**: Инициализирует матрицы весов `W` (векторы слов) и `W_prime` (векторы контекстов).
      - **`forward`**: Прямой проход, который просто извлекает вектор целевого слова.
      - **`backward`**: Реализует обратное распространение ошибки. Здесь используется **Negative Sampling**: для каждого положительного примера `(target, context)` выбирается несколько случайных негативных примеров.
      - **`train`**: Основной цикл обучения, который проходит по батчам данных, выполняет прямой и обратный проход и обновляет веса.
  - **`utils.py`**: Содержит функции для сохранения и загрузки эмбеддингов в формате CSV.
  - **`train.py`**: Центральный скрипт, который связывает все компоненты для запуска полного цикла обучения.
  - **`visualize.py`**: Скрипт для визуализации. Он загружает CSV-файл с эмбеддингами, применяет t-SNE для уменьшения размерности и строит график с подписями слов.
