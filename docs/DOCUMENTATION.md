## Введение

Этот проект представляет собой реализацию модели **Word2Vec** в архитектуре **Skip-gram**, написанную с нуля на чистом NumPy. Он создан для демонстрации фундаментальных принципов обучения векторных представлений слов (эмбеддингов) без использования высокоуровневых фреймворков, таких как TensorFlow или PyTorch. Проект позволяет обучать векторные представления слов на любом текстовом корпусе.

## Описание модулей

### 1\. `src/preprocess.py`

Модуль для подготовки текстовых данных.

#### Класс `Preprocessor`

  - **Переменные**:
      - `vocab_size` (int): Максимальный размер словаря.
      - `lowercase` (bool): Флаг для приведения текста к нижнему регистру.
      - `word2idx` (dict): Словарь, сопоставляющий слово с его индексом.
      - `idx2word` (dict): Словарь, сопоставляющий индекс со словом.
  - **Методы**:
      - `clean_text(text: str) -> str`: Очистка текста от символов, не являющихся буквами.
      - `tokenize(text: str) -> list`: Разделение текста на список слов.
      - `build_vocab(tokens: list)`: Построение словаря на основе самых частых слов. Резервирует индекс `0` для токена `<UNK>`.
      - `encode(tokens: list) -> list`: Преобразование списка слов в список их индексов.
      - `decode(indices: list) -> list`: Преобразование списка индексов обратно в слова.
      - `process_file(filepath: str) -> list`: Полный конвейер обработки текстового файла.

### 2\. `src/dataset.py`

Модуль для создания пар (target, context) для обучения.

#### Класс `SkipGramDataset`

  - **Переменные**:
      - `encoded_text` (list): Список индексов слов.
      - `window_size` (int): Размер контекстного окна.
      - `pairs` (list): Список всех пар (target, context).
  - **Методы**:
      - `__init__(encoded_text, window_size)`: Инициализирует датасет и сразу генерирует все пары.
      - `generate_pairs() -> list`: Создает все пары (target, context) из закодированного текста.
      - `get_batches(batch_size: int) -> generator`: Генератор, который перемешивает пары и выдает их порциями (батчами).

### 3\. `src/model.py`

Ядро проекта. Реализация модели Skip-gram.

#### Класс `SkipGram`

  - **Переменные**:
      - `vocab_size` (int): Размер словаря.
      - `embedding_dim` (int): Размерность векторов слов.
      - `lr` (float): Скорость обучения.
      - `neg_sampling` (bool): Флаг для включения/выключения Negative Sampling.
      - `neg_samples` (int): Количество негативных примеров для сэмплирования.
      - `W` (np.ndarray): Матрица векторов слов. Размер: `(vocab_size, embedding_dim)`.
      - `W_prime` (np.ndarray): Матрица векторов контекстов. Размер: `(embedding_dim, vocab_size)`.
  - **Методы**:
      - `sigmoid(x) -> float`: Функция активации сигмоида.
      - `forward(target_idx: int) -> np.ndarray`: Прямой проход. Возвращает вектор целевого слова.
      - `backward(h: np.ndarray, context_idx: int) -> tuple`: Обратное распространение ошибки. Вычисляет градиенты и обновляет веса. Реализует логику Negative Sampling.
      - `train(dataset, epochs: int)`: Основной цикл обучения. Проходит по датасету, выполняет прямой и обратный проходы и обновляет веса.
      - `get_embedding(word_idx: int) -> np.ndarray`: Возвращает вектор слова по его индексу.

### 4\. `src/utils.py`

Вспомогательные функции для работы с эмбеддингами.

  - **Методы**:
      - `save_embeddings(embeddings, vocab, filepath)`: Сохраняет матрицу эмбеддингов в CSV-файл.
      - `load_embeddings(filepath) -> tuple`: Загружает эмбеддинги из CSV-файла.

### 5\. `src/train.py`

Главный скрипт для запуска обучения.

  - **Методы**:
      - `main()`: Координирует весь процесс: загружает данные, препроцессирует их, создает модель, запускает обучение и сохраняет результаты.

### 6\. `tests/visualize.py`

Скрипт для визуализации обученных эмбеддингов.

  - **Методы**:
      - **main**: Загружает эмбеддинги из файла `artifacts/embeddings.csv`, использует алгоритм **t-SNE** для уменьшения размерности до 2D и строит график.
