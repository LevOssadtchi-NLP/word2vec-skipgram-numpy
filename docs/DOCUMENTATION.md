## Введение

Этот проект представляет собой реализацию модели **Word2Vec** в архитектуре **Skip-gram**, написанную с нуля на чистом NumPy. Он создан для демонстрации фундаментальных принципов обучения векторных представлений слов (эмбеддингов) без использования высокоуровневых фреймворков, таких как TensorFlow или PyTorch. Проект позволяет обучать векторные представления слов на любом текстовом корпусе.

## Математическая основа Skip-gram

Основная идея Skip-gram заключается в том, чтобы **предсказать контекстные слова, находящиеся в окне вокруг целевого слова**. Цель обучения — максимизировать вероятность появления контекстных слов, когда дано целевое слово.

### Функция потерь

Математически это выражается через функцию правдоподобия:

$$L(\\theta) = \\prod\_{w \\in T} \\prod\_{c \\in C(w)} P(c | w)$$

Где:

  - $T$ — корпус текстов.
  - $C(w)$ — набор контекстных слов для слова $w$.
  - $P(c | w)$ — вероятность появления контекстного слова $c$, когда дано целевое слово $w$.

Эту вероятность вычисляют с помощью функции **Softmax**:

$$P(c | w) = \\frac{\\exp(\\mathbf{v}*c^T \\mathbf{v}*w)}{\\sum*{w' \\in V} \\exp(\\mathbf{v}*{w'}^T \\mathbf{v}\_w)}$$

  - $\\mathbf{v}\_w$ — вектор целевого слова $w$.
  - $\\mathbf{v}\_c$ — вектор контекстного слова $c$.
  - $V$ — размер всего словаря.

### Negative Sampling (Оптимизация)

Прямой расчет Softmax очень затратен, так как требует суммирования по всему словарю ($V$). Для решения этой проблемы в модели используется **Negative Sampling**. Вместо того чтобы предсказывать все слова в словаре, мы преобразуем задачу в бинарную классификацию:

1.  **Положительный пример**: Является ли слово $c$ контекстным для слова $w$? (Ответ: "да")
2.  **Негативный пример**: Является ли случайное слово $n$ контекстным для слова $w$? (Ответ: "нет")

Функция потерь для каждого обучающего шага теперь выглядит так:

$$L = -\\log(\\sigma(\\mathbf{v}*c^T \\mathbf{v}*w)) - \\sum*{i=1}^{k} \\log(\\sigma(-\\mathbf{v}*{n\_i}^T \\mathbf{v}\_w))$$

Где:

  - $\\sigma$ — функция сигмоиды.
  - $\\mathbf{v}\_{n\_i}$ — вектор $i$-го негативного примера.
  - $k$ — количество негативных примеров.

Эта оптимизация значительно ускоряет обучение, позволяя модели фокусироваться на нескольких негативных примерах вместо всего словаря.

-----

## Структура проекта

  - `data/`: Содержит текстовые корпуса для обучения (`ruwiki_sample.txt`).
  - `src/`: Основной код проекта.
  - `notebooks/`: Содержит демонстрационные Jupyter-ноутбуки.
  - `tests/`: Скрипты для проверки и визуализации.
  - `requirements.txt`: Список зависимостей Python.
  - `README.md`: Краткое описание проекта.

-----

## Описание модулей

### 1\. `src/preprocess.py`

Модуль для подготовки текстовых данных.

#### Класс `Preprocessor`

  - **Переменные**:
      - `vocab_size` (int): Максимальный размер словаря.
      - `lowercase` (bool): Флаг для приведения текста к нижнему регистру.
      - `word2idx` (dict): Словарь, сопоставляющий слово с его индексом.
      - `idx2word` (dict): Словарь, сопоставляющий индекс со словом.
  - **Методы**:
      - `clean_text(text: str) -> str`: Очистка текста от символов, не являющихся буквами.
      - `tokenize(text: str) -> list`: Разделение текста на список слов.
      - `build_vocab(tokens: list)`: Построение словаря на основе самых частых слов. Резервирует индекс `0` для токена `<UNK>`.
      - `encode(tokens: list) -> list`: Преобразование списка слов в список их индексов.
      - `decode(indices: list) -> list`: Преобразование списка индексов обратно в слова.
      - `process_file(filepath: str) -> list`: Полный конвейер обработки текстового файла.

### 2\. `src/dataset.py`

Модуль для создания пар (target, context) для обучения.

#### Класс `SkipGramDataset`

  - **Переменные**:
      - `encoded_text` (list): Список индексов слов.
      - `window_size` (int): Размер контекстного окна.
      - `pairs` (list): Список всех пар (target, context).
  - **Методы**:
      - `__init__(encoded_text, window_size)`: Инициализирует датасет и сразу генерирует все пары.
      - `generate_pairs() -> list`: Создает все пары (target, context) из закодированного текста.
      - `get_batches(batch_size: int) -> generator`: Генератор, который перемешивает пары и выдает их порциями (батчами).

### 3\. `src/model.py`

Ядро проекта. Реализация модели Skip-gram.

#### Класс `SkipGram`

  - **Переменные**:
      - `vocab_size` (int): Размер словаря.
      - `embedding_dim` (int): Размерность векторов слов.
      - `lr` (float): Скорость обучения.
      - `neg_sampling` (bool): Флаг для включения/выключения Negative Sampling.
      - `neg_samples` (int): Количество негативных примеров для сэмплирования.
      - `W` (np.ndarray): Матрица векторов слов. Размер: `(vocab_size, embedding_dim)`.
      - `W_prime` (np.ndarray): Матрица векторов контекстов. Размер: `(embedding_dim, vocab_size)`.
  - **Методы**:
      - `sigmoid(x) -> float`: Функция активации сигмоида.
      - `forward(target_idx: int) -> np.ndarray`: Прямой проход. Возвращает вектор целевого слова.
      - `backward(h: np.ndarray, context_idx: int) -> tuple`: Обратное распространение ошибки. Вычисляет градиенты и обновляет веса. Реализует логику Negative Sampling.
      - `train(dataset, epochs: int)`: Основной цикл обучения. Проходит по датасету, выполняет прямой и обратный проходы и обновляет веса.
      - `get_embedding(word_idx: int) -> np.ndarray`: Возвращает вектор слова по его индексу.

### 4\. `src/utils.py`

Вспомогательные функции для работы с эмбеддингами.

  - **Методы**:
      - `save_embeddings(embeddings, vocab, filepath)`: Сохраняет матрицу эмбеддингов в CSV-файл.
      - `load_embeddings(filepath) -> tuple`: Загружает эмбеддинги из CSV-файла.

### 5\. `src/train.py`

Главный скрипт для запуска обучения.

  - **Методы**:
      - `main()`: Координирует весь процесс: загружает данные, препроцессирует их, создает модель, запускает обучение и сохраняет результаты.

### 6\. `tests/visualize.py`

Скрипт для визуализации обученных эмбеддингов.

  - **Методы**:
      - **main**: Загружает эмбеддинги из файла `artifacts/embeddings.csv`, использует алгоритм **t-SNE** для уменьшения размерности до 2D и строит график.
